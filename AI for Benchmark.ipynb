{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5fab543-675f-4f19-b34f-e698bca9c1da",
   "metadata": {},
   "source": [
    "# Soc25: AI Perf (Commits -> Benchmarks)\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "| Section Number | Section Title                     | Description                                          |\n",
    "|----------------|---------------------------------|----------------------------------------------------|\n",
    "| 1              | [Data Collection](#markdown-header-data-collection)                 | Fetching and aggregating commit messages from GitHub repositories. |\n",
    "| 2              | [Data Processing & Labeling](#data-processing--labeling)     | Cleaning, encoding labels, and splitting data for training and evaluation. |\n",
    "| 3              | [Model Architecture](#model-architecture)             | Defining the DistilBERT-based classifier network.  |\n",
    "| 4              | [Model Training and Evaluation](#markdown-header-model-training-and-evaluation)     | Iterative training, loss calculation, validation, and saving the best model. |\n",
    "| 5              | [Inference & Testing](#inference--testing)             | Loading the trained model and running predictions on new commit messages. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "072e19ef-0dec-4b16-bf47-0142752623ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # !pip install peft==0.7.1 transformers==4.31.0 accelerate==0.24.1 \n",
    "# !pip uninstall -y accelerate transformers\n",
    "# !pip install --upgrade transformers accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22e8a962-ada3-495c-8c2e-1b763f898563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Name: NVIDIA A100 80GB PCIe\n",
      "CUDA Capability: (8, 0)\n",
      "Memory Total (GB): 84.97\n",
      "Multi-processors: 108\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA Capability:\", torch.cuda.get_device_capability(0))\n",
    "    print(\"Memory Total (GB):\", round(torch.cuda.get_device_properties(0).total_memory / 1e9, 2))\n",
    "    print(\"Multi-processors:\", torch.cuda.get_device_properties(0).multi_processor_count)\n",
    "else:\n",
    "    print(\"No CUDA GPU detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63b57163-e5b6-4957-a94e-ea55cbf5039e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f5468daa15845e3861f4f654817d773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "model_id = \"deepseek-ai/deepseek-coder-6.7b-instruct\"\n",
    "\n",
    "# Load tokenizer and model (use float16 if your GPU supports it)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Avoid warnings\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",           # Automatically place on GPU(s)\n",
    "    torch_dtype=torch.float16,   # Use float16 if possible for efficiency\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.eos_token_id  # For generation padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b80f14fc-d40f-467e-b4e4-9f22a067123a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 3.10 sec\n",
      "Output length: 121\n",
      "Tokens/sec: 39.07\n",
      "<s>[INST] <<SYS>>\n",
      "You are a helpful assistant.\n",
      "<</SYS>>\n",
      "\n",
      "Explain DaCapo in Adoptium.\n",
      "\n",
      "[/INST]\n",
      "\n",
      "<s>[INST] <<SYS>>\n",
      "DaCapo is a benchmark suite that consists of a set of Java applications that have been designed to represent a variety of tasks that are common in real-world applications. It is used to evaluate the performance of a Java virtual machine (JVM), and to measure the efficiency of the JVM's optimizer. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"<s>[INST] <<SYS>>\n",
    "You are a helpful assistant.\n",
    "<</SYS>>\n",
    "\n",
    "Explain DaCapo in Adoptium.\n",
    "\n",
    "[/INST]\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=80,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    end = time.time()\n",
    "\n",
    "print(f\"Elapsed: {end - start:.2f} sec\")\n",
    "print(f\"Output length: {outputs.shape[1]}\")\n",
    "print(f\"Tokens/sec: {outputs.shape[1] / (end - start):.2f}\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc51a712-fdae-4650-9b48-eea4dd7ed3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "training = [\n",
    "    (\"What is DaCapo in Adoptium?\", \"DaCapo is a benchmark suite used by Adoptium to evaluate Java runtime performance across various workloads.\"),\n",
    "    (\"Why is DaCapo important in Adoptium?\", \"DaCapo provides real-world benchmarks that help developers ensure the performance and stability of Java builds in Adoptium.\"),\n",
    "    (\"How does Adoptium use DaCapo?\", \"Adoptium uses DaCapo as part of its QA process to run performance tests and compare runtime behavior across builds.\"),\n",
    "    (\"Name some benchmarks included in DaCapo.\", \"Some of the benchmarks in DaCapo include Eclipse, H2, Luindex, Lusearch, Xalan, and others representing real-world Java applications.\"),\n",
    "    (\"What is the Eclipse benchmark in DaCapo?\", \"The Eclipse benchmark in DaCapo simulates a typical integrated development environment (IDE) workload by running a batch build in the Eclipse Java IDE. It is used to evaluate the performance of compilers and build systems.\"),\n",
    "    (\"What does the H2 benchmark in DaCapo test?\",\"The H2 benchmark evaluates the performance of the H2 Java SQL database engine, testing tasks such as query execution and data manipulation to simulate database workloads.\"),\n",
    "    (\"What is the Luindex benchmark in DaCapo?\",\"Luindex benchmarks the indexing phase of the Lucene search engine. It measures performance when indexing a large set of documents, representing a write-heavy search engine workload.\"),\n",
    "    (\"What does the Lusearch benchmark in DaCapo simulate?\",\"Lusearch represents the search phase of Lucene, focusing on how efficiently a Java application can perform full-text search operations over indexed content.\"),\n",
    "    (\"What kind of workload does the Xalan benchmark in DaCapo represent?\",\"The Xalan benchmark measures the performance of XSLT transformations in Java, simulating applications that convert XML data using XSL stylesheets.\"),\n",
    "    (\"What does the Tradebeans benchmark in DaCapo test?\",\"Tradebeans is a J2EE benchmark in DaCapo that exercises EJB (Enterprise Java Beans) and simulates online stock trading operations, testing application server and transaction performance.\"),\n",
    "    (\"Why are Lucene-based benchmarks included in DaCapo?\",\"Lucene-based benchmarks like Luindex and Lusearch are included because Lucene is a widely used open-source Java search engine. These benchmarks help test indexing and search performance in real-world Java applications.\"),\n",
    "    (\"Is the H2 database used outside benchmarking?\",\"Yes, H2 is a lightweight, embedded Java SQL database widely used in development and testing environments due to its ease of use and fast startup times.\"),\n",
    "    (\"What kind of applications benefit from the Eclipse benchmark data?\",\"Applications involving Java code compilation, build systems, or integrated development environments benefit from Eclipse benchmark data, as it reflects real-world developer usage patterns.</s>\")\n",
    "]\n",
    "\n",
    "with open(\"dacapo_train.jsonl\", \"w\") as f:\n",
    "    for question, answer in training:\n",
    "        entry = {\n",
    "            \"text\": f\"<s>[INST] {question} [/INST] {answer}</s>\"\n",
    "        }\n",
    "        f.write(json.dumps(entry) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e02c2d05-811f-48b9-a0bb-afff282f3b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "425724d5-5170-4641-862d-359b345944bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "488aa3dda7a34cf49645bcd959b426d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36aef82db72e40858bace50d3bd41132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating tokenized dataset...\n",
      "Sample 0: Max token ID = 32021, Vocab size = 32000\n",
      "ERROR: Token ID 32021 still exceeds vocabulary size!\n",
      "Dataset validation complete!\n",
      "Initializing trainer...\n",
      "Error initializing trainer: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Model vocabulary size: 32256\n",
      "Tokenizer vocabulary size: 32000\n",
      "Model pad_token_id: 32021\n",
      "Tokenizer pad_token_id: 32014\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 149\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitializing trainer...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainer initialized successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/transformers/trainer.py:459\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_func \u001b[38;5;241m=\u001b[39m compute_loss_func\n\u001b[1;32m    458\u001b[0m \u001b[38;5;66;03m# Seed must be set before instantiating the model when using model\u001b[39;00m\n\u001b[0;32m--> 459\u001b[0m enable_full_determinism(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mseed) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mfull_determinism \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mset_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhp_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/transformers/trainer_utils.py:106\u001b[0m, in \u001b[0;36mset_seed\u001b[0;34m(seed, deterministic)\u001b[0m\n\u001b[1;32m    104\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed_all(seed)\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# ^^ safe to call this function even if cuda is not available\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/_compile.py:51\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     49\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    836\u001b[0m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback))\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    840\u001b[0m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/random.py:46\u001b[0m, in \u001b[0;36mmanual_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmps\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/cuda/random.py:128\u001b[0m, in \u001b[0;36mmanual_seed_all\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    125\u001b[0m         default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[1;32m    126\u001b[0m         default_generator\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[0;32m--> 128\u001b[0m \u001b[43m_lazy_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_all\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/cuda/__init__.py:302\u001b[0m, in \u001b[0;36m_lazy_call\u001b[0;34m(callable, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _initialization_lock:\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 302\u001b[0m         \u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;66;03m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[39;00m\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;66;03m# file system to get traceback info. Patch linecache or do something\u001b[39;00m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;66;03m# else here if this ends up being important.\u001b[39;00m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28;01mglobal\u001b[39;00m _lazy_seed_tracker\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/cuda/random.py:126\u001b[0m, in \u001b[0;36mmanual_seed_all.<locals>.cb\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(device_count()):\n\u001b[1;32m    125\u001b[0m     default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[0;32m--> 126\u001b[0m     \u001b[43mdefault_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import os\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Set environment variable for better debugging\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "model_id = \"deepseek-ai/deepseek-coder-6.7b-instruct\"\n",
    "\n",
    "# 1. Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. Load model with optimizations\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_safetensors=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Ensure pad_token_id is set consistently\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# 3. Load dataset from your jsonl file\n",
    "dataset = load_dataset(\"json\", data_files={\"train\": \"dacapo_train.jsonl\"})\n",
    "\n",
    "# 4. Fixed tokenization function with proper validation\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the text\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=None,  # Return lists, not tensors\n",
    "    )\n",
    "    \n",
    "    # Validate and fix token IDs\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    fixed_input_ids = []\n",
    "    fixed_attention_mask = []\n",
    "    \n",
    "    for i, input_ids in enumerate(tokenized[\"input_ids\"]):\n",
    "        # Handle None values and out-of-range tokens\n",
    "        if input_ids is None:\n",
    "            input_ids = [tokenizer.bos_token_id, tokenizer.eos_token_id]\n",
    "        \n",
    "        # Fix out-of-range tokens\n",
    "        fixed_ids = []\n",
    "        for token_id in input_ids:\n",
    "            if token_id is None:\n",
    "                fixed_ids.append(tokenizer.unk_token_id)\n",
    "            elif token_id >= vocab_size:\n",
    "                # Token 32013 is likely a special token - replace with EOS\n",
    "                fixed_ids.append(tokenizer.eos_token_id)\n",
    "            else:\n",
    "                fixed_ids.append(token_id)\n",
    "        \n",
    "        fixed_input_ids.append(fixed_ids)\n",
    "        \n",
    "        # Fix attention mask\n",
    "        if tokenized[\"attention_mask\"][i] is None:\n",
    "            fixed_attention_mask.append([1] * len(fixed_ids))\n",
    "        else:\n",
    "            fixed_attention_mask.append(tokenized[\"attention_mask\"][i])\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": fixed_input_ids,\n",
    "        \"attention_mask\": fixed_attention_mask\n",
    "    }\n",
    "\n",
    "# Apply tokenization with debugging\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# 5. Validate the tokenized dataset\n",
    "print(\"Validating tokenized dataset...\")\n",
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "# Check a few samples with proper None handling\n",
    "for i in range(min(3, len(train_dataset))):\n",
    "    sample = train_dataset[i]\n",
    "    input_ids = sample[\"input_ids\"]\n",
    "    \n",
    "    # Handle None values and get max token\n",
    "    if input_ids is None or len(input_ids) == 0:\n",
    "        max_token = 0\n",
    "    else:\n",
    "        # Filter out None values before finding max\n",
    "        valid_tokens = [token for token in input_ids if token is not None]\n",
    "        max_token = max(valid_tokens) if valid_tokens else 0\n",
    "    \n",
    "    print(f\"Sample {i}: Max token ID = {max_token}, Vocab size = {vocab_size}\")\n",
    "    if max_token >= vocab_size:\n",
    "        print(f\"ERROR: Token ID {max_token} still exceeds vocabulary size!\")\n",
    "        break\n",
    "    \n",
    "print(\"Dataset validation complete!\")\n",
    "\n",
    "# 6. Setup training arguments with A100 optimizations\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned-deepseek-coder\",\n",
    "    per_device_train_batch_size=2,  # A100 can handle larger batches\n",
    "    gradient_accumulation_steps=2,   # Effective batch size of 4\n",
    "    num_train_epochs=2,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=10,\n",
    "    bf16=True,  # A100 supports bfloat16\n",
    "    fp16=False,\n",
    "    gradient_checkpointing=True,\n",
    "    learning_rate=5e-6,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    dataloader_pin_memory=True,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\",\n",
    "    seed=42,  # Set explicit seed for reproducibility\n",
    "    data_seed=42,\n",
    "    # Optimization for A100\n",
    "    dataloader_num_workers=4,\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # eval_steps=500,\n",
    ")\n",
    "\n",
    "# 7. Data collator with proper padding\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, \n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=8,\n",
    ")\n",
    "\n",
    "# 8. Initialize Trainer with error handling\n",
    "print(\"Initializing trainer...\")\n",
    "try:\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    print(\"Trainer initialized successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing trainer: {e}\")\n",
    "    # Additional debugging\n",
    "    print(f\"Model vocabulary size: {model.config.vocab_size}\")\n",
    "    print(f\"Tokenizer vocabulary size: {tokenizer.vocab_size}\")\n",
    "    print(f\"Model pad_token_id: {model.config.pad_token_id}\")\n",
    "    print(f\"Tokenizer pad_token_id: {tokenizer.pad_token_id}\")\n",
    "    raise\n",
    "\n",
    "# 9. Train with timing and error handling\n",
    "print(\"Starting training...\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    trainer.train()\n",
    "    end_time = time.time()\n",
    "    print(f\"\\n✅ Training completed in {(end_time - start_time) / 60:.2f} minutes\")\n",
    "except Exception as e:\n",
    "    print(f\"Training error: {e}\")\n",
    "    # Clear CUDA cache and retry with smaller batch size\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Retrying with smaller batch size...\")\n",
    "    training_args.per_device_train_batch_size = 1\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "# 10. Save model\n",
    "print(\"Saving model...\")\n",
    "trainer.save_model(\"./finetuned-deepseek-coder\")\n",
    "tokenizer.save_pretrained(\"./finetuned-deepseek-coder\")\n",
    "print(\"Model saved successfully!\")\n",
    "\n",
    "# 11. Test the model\n",
    "def test_model():\n",
    "    print(\"\\n🧪 Testing fine-tuned model...\")\n",
    "    test_prompt = \"<s>[INST] What is DaCapo in Adoptium? [/INST]\"\n",
    "    inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"Model response: {response}\")\n",
    "\n",
    "# Uncomment to test after training\n",
    "# test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91ba4f28-0deb-49f4-a437-3352a7b94794",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DacForCausalLM' from 'transformers.models.dac' (/data1/phdiep/myenv/lib/python3.10/site-packages/transformers/models/dac/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdac\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DacConfig, DacForCausalLM\n\u001b[1;32m      3\u001b[0m checkpoint_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./finetuned-dacapo\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load config explicitly\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'DacForCausalLM' from 'transformers.models.dac' (/data1/phdiep/myenv/lib/python3.10/site-packages/transformers/models/dac/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers.models.dac import DacConfig, DacForCausalLM\n",
    "\n",
    "checkpoint_path = \"./finetuned-dacapo\"\n",
    "\n",
    "# Load config explicitly\n",
    "config = DacConfig.from_pretrained(checkpoint_path)\n",
    "\n",
    "# Load model with this config\n",
    "model = DacForCausalLM.from_pretrained(checkpoint_path, config=config)\n",
    "\n",
    "model.to(\"cuda\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint_path)\n",
    "model.to(\"cuda\")\n",
    "prompt = \"What is DaCapo?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=50,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    ")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1ab6dc9-9eaa-4db2-adba-d9f18e24a229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab size: 32000\n",
      "Model vocab size: 32256\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizer vocab size:\", tokenizer.vocab_size)\n",
    "print(\"Model vocab size:\", model.config.vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a60969e-140b-4c49-a8ca-2e833f84d293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated(device) / 1024**3:.2f} GB\")\n",
    "print(f\"Cached:    {torch.cuda.memory_reserved(device) / 1024**3:.2f} GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
