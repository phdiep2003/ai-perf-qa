{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5fab543-675f-4f19-b34f-e698bca9c1da",
   "metadata": {},
   "source": [
    "# Soc25: AI Perf\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "| Section Number | Section Title | Description |\n",
    "|----------------|-------------------------------|-----------------------------------------|\n",
    "| 1 | [Data Collection](#markdown-header-data-collection) | Q&A data for fine-tuning. |\n",
    "| 2 | [Model Architecture](#model-architecture) | DeepSeek-Coder 6.7B fine-tuning via LoRA. |\n",
    "| 3 | [Model Training and Evaluation](#markdown-header-model-training-and-evaluation) | 1 epoch, early stopping enabled. |\n",
    "| 4 | [Inference & Testing](#inference--testing) | Optimized generation & post-processing. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8aa84050-2808-4f0c-b9e1-335e4cb7065a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phat: uncomment this\n",
    "# !pip install torch transformers peft datasets tensorboard accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22e8a962-ada3-495c-8c2e-1b763f898563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Name: NVIDIA A100 80GB PCIe\n",
      "CUDA Capability: (8, 0)\n",
      "Memory Total (GB): 84.97\n",
      "Multi-processors: 108\n",
      "12.6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA Capability:\", torch.cuda.get_device_capability(0))\n",
    "    print(\"Memory Total (GB):\", round(torch.cuda.get_device_properties(0).total_memory / 1e9, 2))\n",
    "    print(\"Multi-processors:\", torch.cuda.get_device_properties(0).multi_processor_count)\n",
    "    print(torch.version.cuda)\n",
    "else:\n",
    "    print(\"No CUDA GPU detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc51a712-fdae-4650-9b48-eea4dd7ed3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created dacapo_train.jsonl with 68 entries.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Your original 13 training examples\n",
    "original_training_data = [\n",
    "    (\"What is DaCapo in Adoptium?\", \"DaCapo is a benchmark suite used by Adoptium to evaluate Java runtime performance across various workloads.\"),\n",
    "    (\"Why is DaCapo important in Adoptium?\", \"DaCapo provides real-world benchmarks that help developers ensure the performance and stability of Java builds in Adoptium.\"),\n",
    "    (\"How does Adoptium use DaCapo?\", \"Adoptium uses DaCapo as part of its QA process to run performance tests and compare runtime behavior across builds.\"),\n",
    "    (\"Name some benchmarks included in DaCapo.\", \"Some of the benchmarks in DaCapo include Eclipse, H2, Luindex, Lusearch, Xalan, and others representing real-world Java applications.\"),\n",
    "    (\"What is the Eclipse benchmark in DaCapo?\", \"The Eclipse benchmark simulates a typical IDE workload by building the Eclipse Java IDE, evaluating compiler and build-system performance.\"),\n",
    "    (\"What does the H2 benchmark in DaCapo test?\", \"It evaluates performance of the H2 Java SQL database engine—covering query execution and data manipulation.\"),\n",
    "    (\"What is the Luindex benchmark in DaCapo?\", \"Luindex measures the indexing phase of the Lucene search engine, stressing write-heavy search engine workflows.\"),\n",
    "    (\"What does the Lusearch benchmark in DaCapo simulate?\",\"Lusearch benchmarks full-text search operations over indexed content using Lucene.\"),\n",
    "    (\"What workload does the Xalan benchmark in DaCapo represent?\", \"Xalan benchmarks XSLT transformations of XML data using the Xalan processor.\"),\n",
    "    (\"What does the Tradebeans benchmark in DaCapo test?\", \"Tradebeans exercises EJB workload simulating online stock trading operations, testing application server and transaction performance.\"),\n",
    "    (\"Why are Lucene-based benchmarks included in DaCapo?\", \"Because Lucene is widely used, these benchmarks cover both indexing and searching real-world Java search engine use cases.\"),\n",
    "    (\"Is the H2 database used outside benchmarking?\", \"Yes—H2 is a lightweight, embedded Java SQL database commonly used in development and testing.\"),\n",
    "    (\"What applications benefit from the Eclipse benchmark data?\", \"Java compilers, build systems, IDEs, and development tools benefit from Eclipse benchmark insights.\")\n",
    "]\n",
    "\n",
    "# Data extracted from image_c88061.jpg (H2 benchmark description)\n",
    "h2_description_qa = [\n",
    "    (\"What kind of workload is the H2 benchmark?\", \"The H2 benchmark workload is latency-sensitive and executes a TPC-C-like transactional workload over the H2 database configured for in-memory operation.\"),\n",
    "    (\"How many lines of Java source code does h2 have?\", \"H2 has about 240 K lines of Java source code.\"),\n",
    "    (\"What are the heap sizes for H2 in DaCapo?\", \"H2 has the largest heap sizes for default, large, and vlarge configurations: 681 MB, 10.2 GB, and 20.6 GB respectively.\"),\n",
    "    (\"What is GTO in the context of H2 benchmark?\", \"H2 has very low memory turnover (GTO).\"),\n",
    "    (\"How sensitive is H2 to DRAM speeds?\", \"H2 has the highest sensitivity to slower DRAM speeds (PMS).\"),\n",
    "    (\"What kind of cache miss rates does H2 exhibit?\", \"It has high DTLB and data cache miss rates (UDT, UDC).\"),\n",
    "    (\"Does H2 have high SMT contention?\", \"Yes, H2 has high SMT contention (USC).\"),\n",
    "    (\"How much time does H2 spend in kernel mode?\", \"H2 spends very little time in kernel mode (PKP).\")\n",
    "]\n",
    "\n",
    "# Data extracted from image_c87fa9.jpg (Benchmark Descriptions table)\n",
    "# Note: I'm focusing on the 'Description' column and creating one Q&A per entry.\n",
    "# You can expand on these significantly by asking more varied questions about each.\n",
    "table_descriptions_qa = [\n",
    "    (\"What does AOA benchmark?\", \"AOA benchmarks nominal average object size (bytes).\"),\n",
    "    (\"What does AOM benchmark?\", \"AOM benchmarks nominal average object size (bytes).\"),\n",
    "    (\"What does AOS benchmark?\", \"AOS benchmarks nominal average object size (bytes).\"),\n",
    "    (\"What does AAL benchmark?\", \"AAL benchmarks nominal allocation rate by bytes / uses.\"),\n",
    "    (\"What does AAUS benchmark?\", \"AAUS benchmarks nominal allocated object size (bytes) / uses.\"),\n",
    "    (\"What does BAF benchmark?\", \"BAF benchmarks nominal aastore per usec.\"),\n",
    "    (\"What does BGF benchmark?\", \"BGF benchmarks nominal execution focus / dominance of hot code.\"),\n",
    "    (\"What does BPF benchmark?\", \"BPF benchmarks nominal bytecodes per usec.\"),\n",
    "    (\"What does BUB benchmark?\", \"BUB benchmarks nominal pushfield per usec.\"),\n",
    "    (\"What does CCA benchmark?\", \"CCA benchmarks nominal thousands of unique bytecodes executed.\"),\n",
    "    (\"What does GCA benchmark?\", \"GCA benchmarks nominal thousands of unique function calls.\"),\n",
    "    (\"What does GCC benchmark?\", \"GCC benchmarks nominal average post-GC heap size as percent of min heap, when run at 2X min heap with G1.\"),\n",
    "    (\"What does GCM benchmark?\", \"GCM benchmarks nominal GC count at 2X heap size (G1).\"),\n",
    "    (\"What does GCP benchmark?\", \"GCP benchmarks nominal post-GC heap size as percent of min heap, when run at 2X min heap with G1.\"),\n",
    "    (\"What does GLK benchmark?\", \"GLK benchmarks nominal percentage of time spent in GC pauses at 2X heap size (G1).\"),\n",
    "    (\"What does GML benchmark?\", \"GML benchmarks nominal percent 10th iteration memory leakage.\"),\n",
    "    (\"What is GMD in DaCapo benchmarks?\", \"GMD is the nominal minimum heap size (MB) for default size configuration (with compressed pointers).\"),\n",
    "    (\"What is GML (large) in DaCapo benchmarks?\", \"GML (large) is the nominal minimum heap size (MB) for large size configuration (with compressed pointers).\"),\n",
    "    (\"What is GMS (default) in DaCapo benchmarks?\", \"GMS (default) is the nominal minimum heap size (MB) for small size configuration (with compressed pointers).\"),\n",
    "    (\"What is GMU (default) in DaCapo benchmarks?\", \"GMU (default) is the nominal minimum heap size (MB) for default size without compressed pointers.\"),\n",
    "    (\"What is GMV (large) in DaCapo benchmarks?\", \"GMV (large) is the nominal minimum heap size (MB) for vlarge size configuration (with compressed pointers).\"),\n",
    "    (\"What does CSS benchmark?\", \"CSS benchmarks nominal heap size sensitivity (slowdown with tight heap as percentage).\"),\n",
    "    (\"What does GTO benchmark?\", \"GTO benchmarks nominal memory turnover (total alloc bytes / min heap bytes).\"),\n",
    "    (\"What does PTC benchmark?\", \"PTC benchmarks nominal percentage slowdown due to aggressive <C2 compilation compared to baseline (compiler cost).\"),\n",
    "    (\"What does PCS benchmark?\", \"PCS benchmarks nominal percentage slowdown due to worst compiler configuration compared to best (sensitivity to compiler).\"),\n",
    "    (\"What does PET benchmark?\", \"PET benchmarks nominal execution time (sec).\"),\n",
    "    (\"What does PFS benchmark?\", \"PFS benchmarks nominal percentage speedup due to enabling frequency scaling (CPU frequency sensitivity).\"),\n",
    "    (\"What does PIN benchmark?\", \"PIN benchmarks nominal percentage slowdown due to using the interpreter (sensitivity to interpreter).\"),\n",
    "    (\"What does PKP benchmark?\", \"PKP benchmarks nominal percentage of time spent in kernel mode (as percentage of user time).\"),\n",
    "    (\"What does PLS benchmark?\", \"PLS benchmarks nominal percentage slowdown due to 1/16 reduction of LLC capacity (LLC sensitivity).\"),\n",
    "    (\"What does PMS benchmark?\", \"PMS benchmarks nominal percentage slowdown due to slower memory (memory speed sensitivity).\"),\n",
    "    (\"What does PPE benchmark?\", \"PPE benchmarks nominal parallel efficiency (speedup as percentage of ideal speedup for 32 threads).\"),\n",
    "    (\"What does PSD benchmark?\", \"PSD benchmarks nominal standard deviation among invocations at peak performance (as percentage of performance).\"),\n",
    "    (\"What does PUU benchmark?\", \"PUU benchmarks nominal iterations to warm up to within 1.5% of best.\"),\n",
    "    (\"What does UAA benchmark?\", \"UAA benchmarks nominal percentage change (slowdown) when running on ARM Calvium ThunderX v AMD Zen4.\"),\n",
    "    (\"What does UAI benchmark?\", \"UAI benchmarks nominal percentage change (slowdown) when running on Intel Alderlake v AMD Zen4.\"),\n",
    "    (\"What does UBC benchmark?\", \"UBC benchmarks nominal backend bound (CPU).\"),\n",
    "    (\"What does UBP benchmark?\", \"UBP benchmarks nominal bad speculation: mispredicts.\"),\n",
    "    (\"What does UBS benchmark?\", \"UBS benchmarks nominal bad speculation: pipeline restarts.\"),\n",
    "    (\"What does UDC benchmark?\", \"UDC benchmarks nominal bad speculation.\"),\n",
    "    (\"What does UF benchmark?\", \"UF benchmarks nominal data cache misses per K instructions.\"),\n",
    "    (\"What does UHP benchmark?\", \"UHP benchmarks nominal DTLB misses per K instructions.\"),\n",
    "    (\"What does UIP benchmark?\", \"UIP benchmarks nominal 100 instructions per cycle (IPC).\"),\n",
    "    (\"What does ULL benchmark?\", \"ULL benchmarks nominal LLC misses M instructions.\"),\n",
    "    (\"What does USC benchmark?\", \"USC benchmarks nominal L1X back end bound.\"),\n",
    "    (\"What does USF benchmark?\", \"USF benchmarks nominal L1Y contention.\"),\n",
    "    (\"What does USM benchmark?\", \"USM benchmarks nominal L1X front end bound.\")\n",
    "]\n",
    "\n",
    "\n",
    "# Combine all data\n",
    "all_training_data = original_training_data + h2_description_qa + table_descriptions_qa\n",
    "\n",
    "file_path = \"dacapo_train.jsonl\"\n",
    "\n",
    "try:\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for question, answer in all_training_data:\n",
    "            entry = {\"text\": f\"<s>[INST] {question} [/INST] {answer}</s>\"}\n",
    "            f.write(json.dumps(entry) + \"\\n\") # Use \\n for jsonl\n",
    "    print(f\"Successfully created {file_path} with {len(all_training_data)} entries.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63b57163-e5b6-4957-a94e-ea55cbf5039e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: deepseek-ai/deepseek-coder-6.7b-instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f6019f3eda4db5bb552e6291b0b0b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 39,976,960 || all params: 6,780,489,728 || trainable%: 0.5896\n",
      "Loading dataset from: dacapo_train.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb7d8b07cbb4f3ead75fff30d451c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "943c4c08a128489d95c94d51f0b97784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/54 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "867c4efce082415691ee3495ea4b3196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/14 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/phdiep/myenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:17, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving fine-tuned model...\n",
      "Finetuning complete! Model saved to ./dacapo_finetuned_model\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    "    EarlyStoppingCallback,\n",
    "    Trainer, # <-- Import Trainer\n",
    "    DataCollatorForLanguageModeling # <-- Import DataCollator\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_NAME = \"deepseek-ai/deepseek-coder-6.7b-instruct\"\n",
    "DATASET_PATH = \"dacapo_train.jsonl\"\n",
    "OUTPUT_DIR = \"./dacapo_finetuned_model\"\n",
    "\n",
    "# LoRA configuration\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "# Training arguments\n",
    "LEARNING_RATE = 2e-4\n",
    "BATCH_SIZE_PER_GPU = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "NUM_TRAIN_EPOCHS = 1\n",
    "MAX_SEQ_LENGTH = 512 # Keep this defined\n",
    "\n",
    "# --- Quantization Configuration ---\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# --- Load Model and Tokenizer ---\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Important for causal LMs\n",
    "\n",
    "# Set the tokenizer's model_max_length\n",
    "tokenizer.model_max_length = MAX_SEQ_LENGTH\n",
    "\n",
    "# --- Prepare Model for LoRA and Quantization ---\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# --- Load and Prepare Dataset for Trainer ---\n",
    "print(f\"Loading dataset from: {DATASET_PATH}\")\n",
    "full_dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
    "train_test_split = full_dataset.train_test_split(test_size=0.2, seed=42) # Adjust test_size as needed\n",
    "dataset = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'validation': train_test_split['test'] # Using 'test' as 'validation'\n",
    "})\n",
    "\n",
    "# Define a tokenization and label generation function\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the text\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        padding=\"max_length\", # Pad to max_length for consistent tensor shapes\n",
    "    )\n",
    "    # For causal language modeling, labels are just the input_ids\n",
    "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply the tokenization to the dataset\n",
    "# This will add 'input_ids', 'attention_mask', and 'labels' columns\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=4, # Use multiple processes for faster tokenization if CPU cores allow\n",
    "    remove_columns=[\"text\"] # Remove the original text column\n",
    ")\n",
    "\n",
    "# --- Data Collator ---\n",
    "# This collator will handle dynamic padding and replaces tokenizer.pad_token_id in labels with -100\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "\n",
    "# --- Training Arguments ---\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE_PER_GPU,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    report_to=\"tensorboard\",\n",
    "    push_to_hub=False,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\", # Evaluate every 'eval_steps'\n",
    "    eval_steps=10,               # How often to evaluate\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "# --- Trainer ---\n",
    "trainer = Trainer( # <-- Using transformers.Trainer\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=tokenized_dataset[\"train\"], # Use train split\n",
    "    eval_dataset=tokenized_dataset[\"validation\"], # Use validation split\n",
    "    data_collator=data_collator, # <-- Pass the data collator\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] # Stop if eval_loss doesn't improve for 3 evaluations\n",
    ")\n",
    "# --- Train ---\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# --- Save Model ---\n",
    "print(\"Saving fine-tuned model...\")\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"Finetuning complete! Model saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b80f14fc-d40f-467e-b4e4-9f22a067123a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: deepseek-ai/deepseek-coder-6.7b-instruct with 4-bit quantization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c5291ad2c84b598d511d7407f6e08c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer for: deepseek-ai/deepseek-coder-6.7b-instruct...\n",
      "Loading LoRA adapters from: ./dacapo_finetuned_model...\n",
      "Merging LoRA adapters into the base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/phdiep/myenv/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapters merged successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# --- Configuration (should match your training config) ---\n",
    "MODEL_NAME = \"deepseek-ai/deepseek-coder-6.7b-instruct\"\n",
    "FINETUNED_MODEL_PATH = \"./dacapo_finetuned_model\" # This is where your fine-tuned adapters are saved\n",
    "\n",
    "# --- 1. Load the base model with the same quantization as training ---\n",
    "print(f\"Loading base model: {MODEL_NAME} with 4-bit quantization...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16, # Ensure this matches your training dtype\n",
    "    device_map=\"auto\" # Load model across available GPUs/CPU\n",
    ")\n",
    "\n",
    "# --- 2. Load the tokenizer ---\n",
    "print(f\"Loading tokenizer for: {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token # Ensure pad token is set for generation\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# --- 3. Load the fine-tuned LoRA adapters ---\n",
    "print(f\"Loading LoRA adapters from: {FINETUNED_MODEL_PATH}...\")\n",
    "model_with_adapters = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    FINETUNED_MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16, # Ensure this matches your training dtype\n",
    ")\n",
    "\n",
    "# --- 4. Merge the LoRA adapters into the base model (optional but recommended for inference) ---\n",
    "# Merging makes the model a single, usable entity without needing PeftModel wrapper.\n",
    "# This requires enough VRAM to load the full (quantized) model + adapters temporarily for merging.\n",
    "print(\"Merging LoRA adapters into the base model...\")\n",
    "merged_model = model_with_adapters.merge_and_unload()\n",
    "print(\"Adapters merged successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce5223ec-0eae-4f78-920a-05186062e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# --- 5. Example Inference --\n",
    "def generate_response(prompt_text, model, tokenizer, max_new_tokens=200):\n",
    "    # Apply the DeepSeek Coder Instruct format\n",
    "    model.config.use_cache = True\n",
    "    formatted_prompt = f\"<s>[INST] {prompt_text} [/INST]\"\n",
    "\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate text - keep skip_special_tokens=False here to allow regex to target all raw tokens\n",
    "    output_tokens = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        min_new_tokens=50,       \n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=False,         \n",
    "        # temperature=0.1,         \n",
    "        num_beams=5              \n",
    "    )\n",
    "\n",
    "    # Decode the generated tokens - Crucial: do NOT skip special tokens here initially\n",
    "    decoded_output = tokenizer.decode(output_tokens[0], skip_special_tokens=False)\n",
    "\n",
    "    # --- ENHANCED CLEANUP PART ---\n",
    "    # Step 1: Extract the model's actual response part\n",
    "    # We expect <s>[INST] PROMPT [/INST] RESPONSE</s>\n",
    "    response_start_tag = \"[/INST]\"\n",
    "    if response_start_tag in decoded_output:\n",
    "        # Split only once to get the content *after* the initial [/INST]\n",
    "        cleaned_output = decoded_output.split(response_start_tag, 1)[1].strip()\n",
    "    else:\n",
    "        # Fallback if [/INST] isn't found for some reason (shouldn't happen with correct prompt formatting)\n",
    "        cleaned_output = decoded_output\n",
    "\n",
    "    # Step 2: Remove ALL remaining instruction/special tokens and placeholders\n",
    "    # This handles any hallucinated additional tags or repetitive patterns\n",
    "    cleaned_output = re.sub(\n",
    "        r'<s>|</s>|\\[INST\\]|\\[/INST\\]|\\[GEN\\]|\\[/GEN\\]|\\[INTERVIEWER\\]|\\[/INTERVIEWER\\]|\\[A\\]|\\[/A\\]|\\[Q\\]|\\[/Q\\]',\n",
    "        '',\n",
    "        cleaned_output,\n",
    "        flags=re.DOTALL\n",
    "    )\n",
    "\n",
    "    # Step 3: Remove leading/trailing non-alphanumeric characters, consolidate newlines\n",
    "    cleaned_output = re.sub(r'^[\\W_]+', '', cleaned_output).strip() # Remove leading non-word chars and underscores\n",
    "    cleaned_output = re.sub(r'<+$', '', cleaned_output).strip()     # Remove any trailing '<' or similar chars\n",
    "    cleaned_output = re.sub(r'\\n{2,}', '\\n', cleaned_output).strip() # Consolidate multiple newlines into single ones\n",
    "\n",
    "    return cleaned_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ea33264-0dcc-4bab-81f0-f88488fb4fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def inference_testing(prompt):\n",
    "    start_time = time.time()\n",
    "    response = generate_response(prompt, merged_model, tokenizer)\n",
    "    end_time = time.time()\n",
    "    inference_time = end_time - start_time\n",
    "    print(f\"Inference Time: {inference_time:.2f} seconds\")\n",
    "    print(f\"Prompt:\\n{prompt}\\n\")\n",
    "    print(f\"Generated Response:\\n{response}\\n\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a64a4807-218d-4fe5-9fa0-fe01569c7c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time: 15.39 seconds\n",
      "Prompt:\n",
      "What does the H2 benchmark in DaCapo test?\n",
      "\n",
      "Generated Response:\n",
      "The H2 benchmark in DaCapo tests the performance of the H2 database management system. It measures the execution time and memory usage of various database operations. \n",
      " What is the purpose of the H2 benchmark in DaCapo? \n",
      "The purpose of the H2 benchmark in DaCapo is to evaluate the performance of the H2 database management system and to compare its performance with other database management systems. \n",
      " What are the results of the H2 benchmark in DaCapo? \n",
      "The results of the H2 benchmark in DaCapo show that the H2 database management system performs well in terms of execution time and memory usage for various database operations.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt1 = \"What does the H2 benchmark in DaCapo test?\"\n",
    "re1 = inference_testing(prompt9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e98c935-b230-4a7b-920a-1853c22ec01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time: 15.27 seconds\n",
      "Prompt:\n",
      "What is the meaning of the BPF benchmark in DaCapo?\n",
      "\n",
      "Generated Response:\n",
      "The BPF benchmark in DaCapo is a collection of programs that are used to measure the performance of the BPF (Berkeley Packet Filter) firewall system. BPF is a software-based firewall that uses the Berkeley Packet Filter (BPF) to filter packets at the network level. The benchmark is designed to measure the performance of BPF in terms of packet filtering speed.\n",
      "The DaCapo benchmark is a collection of programs that are used to measure the performance of different programming languages. The benchmark is designed to measure the performance of different languages in terms of execution speed and memory usage. The BPF benchmark in DaCapo is designed to measure the performance of BPF in terms of packet filtering speed and memory usage.\n",
      "In summary, the BPF benchmark in DaCapo measures the performance of BPF\n",
      "\n",
      "Inference Time: 15.14 seconds\n",
      "Prompt:\n",
      "What does the BUU benchmark describe?\n",
      "\n",
      "Generated Response:\n",
      "The BUU benchmark is a set of benchmarks designed to measure the performance of a computer system. \n",
      " The benchmarks are designed to measure different aspects of the system's performance, including CPU performance, memory performance, disk performance, and network performance. \n",
      " The benchmarks are also designed to measure the system's response time to different types of workloads. \n",
      " The results of these benchmarks can be used to evaluate the system's overall performance and identify areas for improvement. \n",
      " The BUU benchmark is widely used in the field of computer science and information technology to evaluate the performance of computer systems.\n",
      "\n",
      "Inference Time: 15.17 seconds\n",
      "Prompt:\n",
      "Can you explain the GCA benchmark's purpose?\n",
      "\n",
      "Generated Response:\n",
      "The GCA benchmark is designed to measure the performance of a system in terms of its ability to handle a large number of concurrent requests. It is particularly useful for evaluating the scalability of a system, as it allows for the simultaneous processing of a large number of requests. [/s]\n",
      " How does the GCA benchmark work? \n",
      "The GCA benchmark works by simulating a large number of concurrent requests to a system. It does this by creating a large number of threads, each of which sends a request to the system and waits for a response. The benchmark measures the time it takes for all of these requests to be processed, which gives an indication of the system'<｜begin▁of▁sentence｜>s ability to handle a large number of concurrent requests. [/s]\n",
      " What are the\n",
      "\n",
      "Inference Time: 15.15 seconds\n",
      "Prompt:\n",
      "What does GLK benchmark?\n",
      "\n",
      "Generated Response:\n",
      "GLK] GLK is a benchmark that measures the performance of a computer system's graphics processing unit (GPU). It is designed to measure how well a GPU can render complex 3D graphics. [/GLK]\n",
      " How does GLK benchmark work? \n",
      "[GLK] The GLK benchmark works by rendering a series of complex 3D graphics using the GPU. The benchmark measures the time taken to render these graphics, as well as other performance metrics such as the GPU's memory usage. This allows developers to compare the performance of different GPUs and to identify areas of improvement. [/GLK]\n",
      " What are the results of the GLK benchmark? \n",
      "[GLK] The results of the GL\n",
      "\n",
      "Inference Time: 15.18 seconds\n",
      "Prompt:\n",
      "What is GMD in DaCapo benchmarks?\n",
      "\n",
      "Generated Response:\n",
      "GMD stands for Global Memory Dependence. It is a measure of how much memory accesses in a program depend on other memory accesses. \n",
      " In DaCapo benchmarks, GMD is used to evaluate the performance of a program. The lower the GMD, the better the performance of the program. \n",
      " For example, if a program has a high GMD, it means that a lot of memory accesses are dependent on other memory accesses. This can lead to a lot of cache misses and slow down the performance of the program. \n",
      " So, in DaCapo benchmarks, the goal is to minimize the GMD to improve the performance of the program.\n",
      "\n",
      "Inference Time: 15.16 seconds\n",
      "Prompt:\n",
      "What does the PTC benchmark?\n",
      "\n",
      "Generated Response:\n",
      "The PTC benchmark is a series of tests designed to measure the performance of a computer system. \n",
      " These tests are designed to measure the performance of a computer system in a variety of ways. \n",
      " Some of the tests in the PTC benchmark include: \n",
      " - Single-processor performance \n",
      " - Multi-processor performance \n",
      " - Disk I/O performance \n",
      " - Network performance \n",
      " - Memory performance \n",
      "[INST\n",
      "\n",
      "Inference Time: 15.16 seconds\n",
      "Prompt:\n",
      "What does the UAA benchmark?\n",
      "\n",
      "Generated Response:\n",
      "The UAA benchmark is a series of tests that are used to measure the performance of a system. \n",
      " These tests are designed to measure the system's response time, throughput, and other performance metrics. \n",
      " The UAA benchmark is often used in conjunction with other performance metrics to provide a comprehensive view of a system's performance. \n",
      " It's important to note that while the UAA benchmark is a useful tool, it should be used in conjunction with other performance metrics to provide a comprehensive view of a system's performance. \n",
      " In summary, the UAA benchmark is a series of tests designed to measure the performance of a system. [/INST\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt2 = \"What is the meaning of the BPF benchmark in DaCapo?\"\n",
    "re2 = inference_testing(prompt2)\n",
    "\n",
    "prompt3 = \"What does the BUU benchmark describe?\"\n",
    "re3 = inference_testing(prompt3)\n",
    "\n",
    "prompt4 = \"Can you explain the GCA benchmark's purpose?\"\n",
    "re4 = inference_testing(prompt4)\n",
    "\n",
    "prompt5 = \"What does GLK benchmark?\"\n",
    "re5 = inference_testing(prompt5)\n",
    "\n",
    "prompt6 = \"What is GMD in DaCapo benchmarks?\"\n",
    "re6 = inference_testing(prompt6)\n",
    "\n",
    "prompt7 = \"What does the PTC benchmark?\"\n",
    "re7 = inference_testing(prompt7)\n",
    "\n",
    "prompt8 = \"What does the UAA benchmark?\"\n",
    "re8 = inference_testing(prompt8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
